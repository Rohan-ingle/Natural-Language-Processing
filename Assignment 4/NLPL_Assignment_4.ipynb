{"cells":[{"cell_type":"markdown","metadata":{},"source":["Name : `Rohan Ingle`        \n","\n","PRN: `22070126047`      \n","\n","Branch: `AIML A2 2022-2026`"]},{"cell_type":"markdown","metadata":{},"source":["### GitHub Link : [https://github.com/Rohan-ingle/Natural-Language-Processing](https://github.com/Rohan-ingle/Natural-Language-Processing)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:55:33.876491Z","iopub.status.busy":"2024-10-17T11:55:33.875385Z","iopub.status.idle":"2024-10-17T11:55:33.881511Z","shell.execute_reply":"2024-10-17T11:55:33.880274Z","shell.execute_reply.started":"2024-10-17T11:55:33.876435Z"},"id":"U-f1BA7Chamr","trusted":true},"outputs":[],"source":["# !pip install pandas nltk scikit-learn rouge -qq"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Necessary Libraries"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:55:33.884025Z","iopub.status.busy":"2024-10-17T11:55:33.883730Z","iopub.status.idle":"2024-10-17T11:55:33.892583Z","shell.execute_reply":"2024-10-17T11:55:33.891715Z","shell.execute_reply.started":"2024-10-17T11:55:33.883994Z"},"id":"_WuxAHxGY_6C","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from rouge import Rouge\n","import os\n","from collections import Counter\n","import nltk\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt', quiet=True)\n","#nltk.download('punkt_tab', quiet=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Model Architecture"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:55:33.894679Z","iopub.status.busy":"2024-10-17T11:55:33.893793Z","iopub.status.idle":"2024-10-17T11:55:33.909294Z","shell.execute_reply":"2024-10-17T11:55:33.908412Z","shell.execute_reply.started":"2024-10-17T11:55:33.894624Z"},"id":"mCIhYCOdY_6E","trusted":true},"outputs":[],"source":["class BiLSTMSummarizer(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(BiLSTMSummarizer, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = src.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.fc.out_features\n","\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n","\n","        embedded = self.embedding(src)\n","        enc_output, (hidden, cell) = self.encoder(embedded)\n","\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n","        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n","\n","        input = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            input_embedded = self.embedding(input).unsqueeze(1)\n","            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n","            prediction = self.fc(output.squeeze(1))\n","            outputs[:, t] = prediction\n","\n","            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n","            top1 = prediction.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["# Making Functions to Parse Dataset, tokenize and build vocaboulary"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:55:33.910604Z","iopub.status.busy":"2024-10-17T11:55:33.910314Z","iopub.status.idle":"2024-10-17T11:55:33.919392Z","shell.execute_reply":"2024-10-17T11:55:33.918558Z","shell.execute_reply.started":"2024-10-17T11:55:33.910574Z"},"id":"tgn_dCLbY_6H","trusted":true},"outputs":[],"source":["def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    return df[\"Content\"].tolist(), df[\"Headline\"].tolist()\n","\n","def tokenize(text):\n","    return word_tokenize(text.lower())\n","\n","def build_vocab(texts, min_freq=2):\n","    word_freq = Counter()\n","    for text in texts:\n","        word_freq.update(text)\n","\n","    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n","    for word, freq in word_freq.items():\n","        if freq >= min_freq:\n","            vocab[word] = len(vocab)\n","\n","    return vocab, {v: k for k, v in vocab.items()}"]},{"cell_type":"markdown","metadata":{},"source":["# Loading The Data"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:55:33.922201Z","iopub.status.busy":"2024-10-17T11:55:33.921685Z","iopub.status.idle":"2024-10-17T11:58:04.859114Z","shell.execute_reply":"2024-10-17T11:58:04.858264Z","shell.execute_reply.started":"2024-10-17T11:55:33.922160Z"},"id":"lHfREZHwY_6I","trusted":true},"outputs":[],"source":["articles, summaries = load_data(\"/kaggle/input/hindi-summarization/hindi_news_dataset.csv\")\n","\n","tokenized_articles = [tokenize(article) for article in articles]\n","tokenized_summaries = [tokenize(summary) for summary in summaries]\n","\n","vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n","\n","train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n","train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["# Class to Initialize and preprocess the Dataset"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:04.861037Z","iopub.status.busy":"2024-10-17T11:58:04.860720Z","iopub.status.idle":"2024-10-17T11:58:04.883557Z","shell.execute_reply":"2024-10-17T11:58:04.882700Z","shell.execute_reply.started":"2024-10-17T11:58:04.861003Z"},"id":"gBEvTN3KY_6F","trusted":true},"outputs":[],"source":["class SummarizationDataset(Dataset):\n","    def __init__(self, articles, summaries, vocab, max_length=50):\n","        self.articles = articles\n","        self.summaries = summaries\n","        self.vocab = vocab\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.articles)\n","\n","    def __getitem__(self, idx):\n","        article = self.articles[idx]\n","        summary = self.summaries[idx]\n","\n","        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n","        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n","\n","        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n","        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n","\n","        return torch.tensor(article_indices), torch.tensor(summary_indices)"]},{"cell_type":"markdown","metadata":{},"source":["# Generating The Dataset fomr loaded data"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:04.885346Z","iopub.status.busy":"2024-10-17T11:58:04.884939Z","iopub.status.idle":"2024-10-17T11:58:04.899871Z","shell.execute_reply":"2024-10-17T11:58:04.898836Z","shell.execute_reply.started":"2024-10-17T11:58:04.885302Z"},"id":"CP18TCZfY_6J","trusted":true},"outputs":[],"source":["train_dataset = SummarizationDataset(train_articles, train_summaries, vocab)\n","val_dataset = SummarizationDataset(val_articles, val_summaries, vocab)\n","test_dataset = SummarizationDataset(test_articles, test_summaries, vocab)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64)\n","test_loader = DataLoader(test_dataset, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Hyperparameters"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:04.902674Z","iopub.status.busy":"2024-10-17T11:58:04.902293Z","iopub.status.idle":"2024-10-17T11:58:05.306515Z","shell.execute_reply":"2024-10-17T11:58:05.305560Z","shell.execute_reply.started":"2024-10-17T11:58:04.902633Z"},"id":"-X7pLGHsY_6J","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs\n"]}],"source":["vocab_size = len(vocab)\n","embedding_dim = 128\n","hidden_dim = 256\n","output_dim = vocab_size\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim)\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs\")\n","    model = nn.DataParallel(model)\n","    \n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Training Function"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:05.308264Z","iopub.status.busy":"2024-10-17T11:58:05.307835Z","iopub.status.idle":"2024-10-17T11:58:05.316558Z","shell.execute_reply":"2024-10-17T11:58:05.315589Z","shell.execute_reply.started":"2024-10-17T11:58:05.308200Z"},"id":"UZGJGt5-Y_6K","trusted":true},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in tqdm(iterator, desc=\"Training\"):\n","        src, trg = batch\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg, teacher_forcing_ratio)\n","\n","        output_dim = output.shape[-1]\n","        output = output[:, 1:].reshape(-1, output_dim)\n","        trg = trg[:, 1:].reshape(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Evaluation Function"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:05.318518Z","iopub.status.busy":"2024-10-17T11:58:05.317930Z","iopub.status.idle":"2024-10-17T11:58:05.331567Z","shell.execute_reply":"2024-10-17T11:58:05.330581Z","shell.execute_reply.started":"2024-10-17T11:58:05.318475Z"},"id":"sUtmmZLIY_6L","trusted":true},"outputs":[],"source":["def evaluate(model, iterator, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in tqdm(iterator, desc=\"Evaluating\"):\n","            src, trg = batch\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0) \n","\n","            output_dim = output.shape[-1]\n","            output = output[:, 1:].reshape(-1, output_dim)\n","            trg = trg[:, 1:].reshape(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Beam Search Function"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Beam search is a heuristic search algorithm that explores a graph by expanding the most promising nodes, maintaining a fixed number of best candidates (beam width) at each step to find the most likely sequence.**"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:05.333347Z","iopub.status.busy":"2024-10-17T11:58:05.332971Z","iopub.status.idle":"2024-10-17T11:58:05.368375Z","shell.execute_reply":"2024-10-17T11:58:05.367502Z","shell.execute_reply.started":"2024-10-17T11:58:05.333306Z"},"id":"dFZaxsetY_6L","trusted":true},"outputs":[],"source":["def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=50, min_length=10, device='cpu'):\n","    model.eval()\n","    \n","    if isinstance(model, nn.DataParallel):\n","        model = model.module\n","    \n","    with torch.no_grad():\n","        # Embedding the input sequence\n","        embedded = model.embedding(src)\n","        enc_output, (hidden, cell) = model.encoder(embedded)\n","\n","        if model.encoder.bidirectional:\n","            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n","            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)\n","        else:\n","            hidden = hidden[-1, :, :]\n","            cell = cell[-1, :, :]\n","\n","        hidden = hidden.unsqueeze(0)\n","        cell = cell.unsqueeze(0)\n","\n","        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]\n","        complete_hypotheses = []\n","\n","        for t in range(max_length):\n","            new_beam = []\n","            for seq, score, hidden, cell in beam:\n","                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n","                    complete_hypotheses.append((seq, score))\n","                    continue\n","\n","                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)\n","                input_embedded = model.embedding(input)\n","\n","                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))\n","                predictions = model.fc(output.squeeze(1))\n","\n","                if len(seq) < min_length:\n","                    predictions[0][vocab['<eos>']] = float('-inf')\n","\n","                top_preds = torch.topk(predictions, beam_width, dim=1)\n","\n","                for i in range(beam_width):\n","                    new_seq = seq + [top_preds.indices[0][i].item()]\n","                    new_score = score - top_preds.values[0][i].item()\n","                    new_hidden = hidden.clone()\n","                    new_cell = cell.clone()\n","                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n","\n","            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n","\n","            if len(complete_hypotheses) >= beam_width:\n","                break\n","\n","        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n","        if complete_hypotheses:\n","            best_seq = complete_hypotheses[0][0]\n","        else:\n","            best_seq = beam[0][0]\n","\n","    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Defining a function to save model at specific instances"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:05.370477Z","iopub.status.busy":"2024-10-17T11:58:05.369717Z","iopub.status.idle":"2024-10-17T11:58:05.383085Z","shell.execute_reply":"2024-10-17T11:58:05.382253Z","shell.execute_reply.started":"2024-10-17T11:58:05.370433Z"},"id":"iYMa8KYbY_6M","trusted":true},"outputs":[],"source":["def save_model(model, vocab, filepath, embedding_dim, hidden_dim, output_dim):\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'vocab': vocab,\n","        'embedding_dim': embedding_dim,\n","        'hidden_dim': hidden_dim,\n","        'output_dim': output_dim\n","    }, filepath)\n","    print(f\"Model saved to {filepath}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training the model"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T11:58:05.397993Z","iopub.status.busy":"2024-10-17T11:58:05.397699Z","iopub.status.idle":"2024-10-17T17:57:07.489334Z","shell.execute_reply":"2024-10-17T17:57:07.488279Z","shell.execute_reply.started":"2024-10-17T11:58:05.397963Z"},"id":"WyyssAHKY_6M","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/2087 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","Training: 100%|██████████| 2087/2087 [34:00<00:00,  1.02it/s]\n","Evaluating: 100%|██████████| 232/232 [01:57<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 5.241\n","\t Val. Loss: 4.580\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:52<00:00,  1.03it/s]\n","Evaluating: 100%|██████████| 232/232 [01:56<00:00,  1.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 02\n","\tTrain Loss: 2.947\n","\t Val. Loss: 3.413\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:55<00:00,  1.03it/s]\n","Evaluating: 100%|██████████| 232/232 [01:57<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 03\n","\tTrain Loss: 1.996\n","\t Val. Loss: 2.777\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:55<00:00,  1.03it/s]\n","Evaluating: 100%|██████████| 232/232 [01:57<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 04\n","\tTrain Loss: 1.491\n","\t Val. Loss: 2.415\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:57<00:00,  1.02it/s]\n","Evaluating: 100%|██████████| 232/232 [01:56<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 05\n","\tTrain Loss: 1.173\n","\t Val. Loss: 2.154\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:52<00:00,  1.03it/s]\n","Evaluating: 100%|██████████| 232/232 [01:57<00:00,  1.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 06\n","\tTrain Loss: 0.952\n","\t Val. Loss: 1.961\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:58<00:00,  1.02it/s]\n","Evaluating: 100%|██████████| 232/232 [01:56<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 07\n","\tTrain Loss: 0.798\n","\t Val. Loss: 1.812\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:54<00:00,  1.03it/s]\n","Evaluating: 100%|██████████| 232/232 [01:56<00:00,  1.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 08\n","\tTrain Loss: 0.680\n","\t Val. Loss: 1.696\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:59<00:00,  1.02it/s]\n","Evaluating: 100%|██████████| 232/232 [01:56<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 09\n","\tTrain Loss: 0.592\n","\t Val. Loss: 1.632\n","Model saved to best_model.pth\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 2087/2087 [33:59<00:00,  1.02it/s]\n","Evaluating: 100%|██████████| 232/232 [01:57<00:00,  1.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10\n","\tTrain Loss: 0.519\n","\t Val. Loss: 1.550\n","Model saved to best_model.pth\n"]}],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n","\n","num_epochs = 10\n","best_val_loss = float('inf')\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    val_loss = evaluate(model, val_loader, criterion, device)\n","    print(f'Epoch: {epoch+1:02}')\n","    print(f'\\tTrain Loss: {train_loss:.3f}')\n","    print(f'\\t Val. Loss: {val_loss:.3f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        save_model(model, vocab, 'best_model.pth', embedding_dim, hidden_dim, output_dim)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Defining a function to load model from checkpoints"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T17:57:07.490883Z","iopub.status.busy":"2024-10-17T17:57:07.490560Z","iopub.status.idle":"2024-10-17T17:57:07.497895Z","shell.execute_reply":"2024-10-17T17:57:07.497013Z","shell.execute_reply.started":"2024-10-17T17:57:07.490849Z"},"id":"3Di3pobeY_6N","trusted":true},"outputs":[],"source":["def load_model(filepath, device):\n","    checkpoint = torch.load(filepath, map_location=device)\n","    vocab = checkpoint['vocab']\n","    \n","    embedding_dim = checkpoint['embedding_dim']\n","    hidden_dim = checkpoint['hidden_dim']\n","    output_dim = checkpoint['output_dim']\n","    vocab_size = len(vocab)\n","\n","    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n","\n","    if torch.cuda.device_count() > 1:\n","        model = nn.DataParallel(model)\n","\n","    # Load the model state\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    \n","    return model, vocab\n"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T17:57:07.499815Z","iopub.status.busy":"2024-10-17T17:57:07.499170Z","iopub.status.idle":"2024-10-17T18:02:38.343105Z","shell.execute_reply":"2024-10-17T18:02:38.342123Z","shell.execute_reply.started":"2024-10-17T17:57:07.499772Z"},"id":"DPyf-nlSY_6N","outputId":"4bb48d10-785b-4019-c8b5-dddc650e6a43","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1194031576.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(filepath, map_location=device)\n","Evaluating: 100%|██████████| 580/580 [04:51<00:00,  1.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 1.546\n"]},{"name":"stderr","output_type":"stream","text":["Generating summaries: 100%|██████████| 580/580 [00:38<00:00, 15.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ROUGE scores:\n","{'rouge-1': {'r': 0.8848763375723716, 'p': 0.8715314577300806, 'f': 0.8749202841986269}, 'rouge-2': {'r': 0.8253247433147615, 'p': 0.7764323152900745, 'f': 0.7971380303678042}, 'rouge-l': {'r': 0.8734796275202089, 'p': 0.8588486369137195, 'f': 0.8630128449535202}}\n"]}],"source":["best_model, _ = load_model('best_model.pth', device)\n","\n","test_loss = evaluate(best_model, test_loader, criterion, device)\n","print(f'Test Loss: {test_loss:.3f}')\n","\n","# Evaluate using ROUGE score\n","rouge = Rouge()\n","best_model.eval()\n","predictions = []\n","references = []\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n","        src, trg = batch\n","        src = src.to(device)\n","        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n","        predictions.extend([' '.join(pred)])\n","        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n","\n","min_length = 10\n","predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n","\n","scores = rouge.get_scores(predictions, references, avg=True)\n","print(\"ROUGE scores:\")\n","print(scores)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:02:38.344679Z","iopub.status.busy":"2024-10-17T18:02:38.344351Z","iopub.status.idle":"2024-10-17T18:02:38.556018Z","shell.execute_reply":"2024-10-17T18:02:38.555067Z","shell.execute_reply.started":"2024-10-17T18:02:38.344646Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/4165049954.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load('best_model.pth', map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["Checkpoint Keys: dict_keys(['model_state_dict', 'vocab', 'embedding_dim', 'hidden_dim', 'output_dim'])\n"]}],"source":["checkpoint = torch.load('best_model.pth', map_location=device)\n","print(\"Checkpoint Keys:\", checkpoint.keys())"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:02:38.557539Z","iopub.status.busy":"2024-10-17T18:02:38.557162Z","iopub.status.idle":"2024-10-17T18:02:39.146217Z","shell.execute_reply":"2024-10-17T18:02:39.145218Z","shell.execute_reply.started":"2024-10-17T18:02:38.557505Z"},"id":"Y68PKEeYY_6N","outputId":"92d0777f-cd84-4b63-efb5-8a0e8a4b2447","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-trained model...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1194031576.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(filepath, map_location=device)\n"]}],"source":["print(\"Loading pre-trained model...\")\n","trained_model, vocab = load_model('best_model.pth', device)\n","inv_vocab = {v: k for k, v in vocab.items()}\n","trained_model = trained_model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Getting summaries"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:02:39.147866Z","iopub.status.busy":"2024-10-17T18:02:39.147471Z","iopub.status.idle":"2024-10-17T18:02:39.155647Z","shell.execute_reply":"2024-10-17T18:02:39.154584Z","shell.execute_reply.started":"2024-10-17T18:02:39.147822Z"},"id":"CMv-l-VmY_6O","trusted":true},"outputs":[],"source":["def summarize_text(model, vocab, inv_vocab, text, max_length=50, min_length=10, beam_width=3, device='cpu', debug=False):\n","    model.eval()\n","    tokens = tokenize(text)[:max_length]\n","    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n","    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n","\n","    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n","\n","    if debug:\n","        print(\"Input tokens:\", tokens)\n","        print(\"Input indices:\", indices)\n","        print(\"Generated indices:\", [vocab[word] for word in summary])\n","        print(\"Summary length:\", len(summary))\n","\n","    return ' '.join(summary)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T18:02:39.157346Z","iopub.status.busy":"2024-10-17T18:02:39.156965Z","iopub.status.idle":"2024-10-17T18:02:39.239945Z","shell.execute_reply":"2024-10-17T18:02:39.239107Z","shell.execute_reply.started":"2024-10-17T18:02:39.157303Z"},"id":"-6HLvvrkY_6O","outputId":"b4210c5f-ccef-496b-9339-27f7b8ec863a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112*', 'रनों', 'की', 'साझेदारी']\n","Input indices: [2, 4910, 37, 8412, 14, 193, 7443, 14, 2425, 2426, 10, 8413, 86, 7167, 96, 67, 1330, 1254, 14, 2425, 2426, 12, 189, 274, 8414, 50, 4910, 8, 725, 7443, 2940, 35, 4910, 71, 7770, 8415, 7479, 14, 8416, 428, 596, 7627, 7628, 43, 8417, 8418, 8, 8419, 7930, 8, 3882, 3]\n","Generated indices: [4910, 37, 1330, 1074, 14, 725, 2710, 2425, 2426, 10, 274, 8414, 50, 7443, 2524, 14, 7751, 50, 900]\n","Summary length: 19\n","Generated Summary:\n","ऑस्ट्रेलिया ने 12 साल में पहली बार दक्षिण अफ्रीका को उसकी धरती पर वनडे मैच में हराया पर पहुंचा\n","Summary length: 19\n"]}],"source":["input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n","summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n","print(\"Generated Summary:\")\n","print(summary)\n","print(\"Summary length:\", len(summary.split()))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5890249,"sourceId":9645024,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
